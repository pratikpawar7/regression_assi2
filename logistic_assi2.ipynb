{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292810-3b72-4b96-bf8b-53b1cfe7d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "'''\n",
    "Gird search is the method used to find the best setting hyperparameter for the ML model , \n",
    "IT tries all possible combinations of the setting and choose one who gaves the best result. \n",
    "\n",
    "\n",
    "Grid Search CV systematically tests all combinations of specified hyperparameters to find the best set for a model. \n",
    "It performs k-fold cross-validation for each combination to evaluate performance, averaging results to\n",
    "determine the best hyperparameters. \n",
    "This thorough search ensures optimal model performance but can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8338d-265d-4fdd-b7ed-756cd4462870",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "'''\n",
    "\n",
    "In Grid search cv there are check all possible combinations of the specified hyperparameter ,  to find the best \n",
    "set of the model . \n",
    "but it requies a lot of time hense \n",
    "There is the Randomized search cv is another method for tunning hyperparameter of ML model ,\n",
    "Instad of tring all possible combinations like Grid search it is randomly selected combination of hyperparameter to try \n",
    "\n",
    "\n",
    "When to Choose:\n",
    "Grid Search CV: Use when the hyperparameter grid is small and computational resources are sufficient.\n",
    "Randomized Search CV: Use when the hyperparameter grid is large or when you need a quicker, more efficient search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d0b43-7c58-4013-9919-13113eb8c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "'''\n",
    "Data leakage occurs when information from outside the training dataset is used to create the model,\n",
    "leading to overly optimistic performance estimates during training but poor performance on new, unseen data.\n",
    "\n",
    "Why It's a Problem:\n",
    "Overfitting: The model learns patterns it shouldn't have access to, leading to poor generalization.\n",
    "Misleading Metrics: Evaluation metrics become overly optimistic, giving a false sense of model performance.\n",
    "Poor Generalization: The model performs well on training data but poorly on new data.\n",
    "\n",
    "Example:\n",
    "In a credit risk model, including a feature like loan_repaid (whether the loan was repaid or not) would\n",
    "directly indicate the target variable, causing the model to learn from future information it wouldn't \n",
    "have at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227db1c-f154-492d-a5ae-fee9cdda5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "'''\n",
    "Preventing Data Leakage:\n",
    "\n",
    "Careful Data Preparation: Ensure features available at prediction time are included.\n",
    "Proper Data Splitting: Split data into training and test sets before any preprocessing.\n",
    "Feature Selection: Avoid features that contain future information or directly imply the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15a095-1125-42e9-a00c-9ea07388291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "'''\n",
    "               Predicted Positive   Predicted Negative\n",
    "Actual Positive        TP                   FN\n",
    "Actual Negative        FP                   TN\n",
    "\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model\n",
    "by comparing the actual and predicted classes.\n",
    "\n",
    "Components:\n",
    "True Positives (TP): Correctly predicted positive instances.\n",
    "True Negatives (TN): Correctly predicted negative instances.\n",
    "False Positives (FP): Incorrectly predicted positive instances (Type I error).\n",
    "False Negatives (FN): Incorrectly predicted negative instances (Type II error).\n",
    "\n",
    "What It Tells You:\n",
    "Accuracy: Proportion of correct predictions (TP + TN) / (TP + TN + FP + FN).\n",
    "Precision: Proportion of positive predictions that are correct TP / (TP + FP).\n",
    "Recall (Sensitivity): Proportion of actual positives correctly identified TP / (TP + FN).\n",
    "F1 Score: Harmonic mean of precision and recall, useful for imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48faf0-de0c-413a-8d47-cc61eae56fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "'''\n",
    "Precision: Proportion of positive predictions that are correct :.\n",
    "                   TP \n",
    "Precision =    --------------\n",
    "                 (TP + FP).\n",
    "    \n",
    "\n",
    "\n",
    "Recall (Sensitivity): Proportion of actual positives correctly identified TP / (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be770e-684c-4843-8963-6138a9619675",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "'''\n",
    "\n",
    "To interpret a confusion matrix and determine the types of errors your model is making, \n",
    "you need to look at the values of True Positives (TP), True Negatives (TN), \n",
    "False Positives (FP), and False Negatives (FN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21abc3-b1e6-466e-a032-4777a676341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "'''\n",
    "Accuracy: Proportion of correct predictions (TP + TN) / (TP + TN + FP + FN).\n",
    "Precision: Proportion of positive predictions that are correct TP / (TP + FP).\n",
    "Recall (Sensitivity): Proportion of actual positives correctly identified TP / (TP + FN).\n",
    "F1 Score: Harmonic mean of precision and recall, useful for imbalanced classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b1824-2868-4b6c-b715-82426e2725fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "'''\n",
    "\n",
    "Accuracy give us information about how accurate the data is. \n",
    "The accuracy of a model is a measure of how many predictions it got right out of all predictions made. \n",
    "\n",
    "accuracy is directly related to the values in the confusion matrix, reflecting the proportion\n",
    "of correct predictions (both positives and negatives) out of the total predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182ca38-9699-4b64-8075-b096af5f1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "'''\n",
    "\n",
    "A confusion matrix helps identify model biases by showing misclassifications.\n",
    "If a class has many false positives or false negatives, it indicates a potential bias. \n",
    "By calculating precision and recall for each class, you can assess how well the model detects positives and negatives.\n",
    "Low precision or recall points to areas where the model might be struggling, revealing \n",
    "possible biases or limitations in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
